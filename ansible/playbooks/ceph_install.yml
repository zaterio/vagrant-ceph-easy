---
- hosts: all
  remote_user: root

  vars_files:
   - ../../vars/vars.yml    
    
  tasks:
 
   - debug: msg="CephAdmNode {{CephAdmNode}} CephMonNode {% for host in CephMonNode %}{{ host }}{%- if not loop.last -%},{% endif %}{% endfor %} CephAllNode {% for host in CephAllNode %}{{ host }}{%- if not loop.last -%},{% endif %}{% endfor %} CephOsdNode {% for host in CephOsdNode %}{{ host }}{%- if not loop.last -%},{% endif %}{% endfor %}"         
          
   - name: add ceph release key
     apt_key:
      url: "https://download.ceph.com/keys/release.asc"
      state: present
     when: not dev
     
   - name: add ceph repo 
     apt_repository:
      repo: deb https://download.ceph.com/debian-{{ceph_stable_release}} xenial main 
      state: present
      filename: 'ceph'
     when: not dev
     
   - name: Update repositories cache and install "ceph-deploy" package
     apt:
      name: ceph-deploy
      update_cache: yes 
     when: not dev
          
   - name: Crea directorios
     become_user: "{{ cephuser }}" 
     file:
      path: /home/{{cephuser}}/{{ cluster_name }}
      state: directory
     when: not dev
 
 
   - name: ceph-deploy disk zap {% for c in cluster %} {% if c.osdnode is defined %}{% for osd in c.osdnode %}{% if osd.dev is defined %}{{ c.node }}:{{osd.dev}}{% if not loop.last %} {% endif %}{% endif %}{% endfor %}{% endif %}{% endfor %}
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     shell: ceph-deploy disk zap {% for c in cluster %} {% if c.osdnode is defined %}{% for osd in c.osdnode %}{% if osd.dev is defined %}{{ c.node }}:{{osd.dev}}{% if not loop.last %} {% endif %}{% endif %}{% endfor %}{% endif %}{% endfor %}
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: purge == true
 
 
 
      
   - name: ceph deploy purgedata {% for host in CephAllNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     shell: ceph-deploy purgedata {% for host in CephAllNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: purge == true
     
   - name: ceph deploy forgetkeys
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     shell: ceph-deploy forgetkeys
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: purge == true
  
    # Start deploying a new cluster and write a configuration file and keyring for it. 
    # It tries to copy ssh keys from admin node to gain passwordless ssh to monitor node(s).
    # validates host IP, creates a cluster with a new initial monitor node or nodes for monitor quorum
    # a ceph configuration file, a monitor secret keyring and a log file for the new cluster. 
    # It populates the newly created Ceph configuration file with fsid of cluster, 
    # hostnames and IP addresses of initial monitor members under [global] section.
    # Other options like --no-ssh-copykey, --fsid, --cluster-network and --public-network can also be used with this command  .
    # If more than one network interface is used, public network setting has to be added under [global] section of Ceph configuration file. 
    # If the public subnet is given, new command will choose the one IP from the remote host that exists within the subnet range. 
    # Public network can also be added at runtime using --public-network option with the command as mentioned above.
   
   - name: ceph-deploy --cluster {{ cluster_name }} new --public-network {{ public_network }} {% for host in CephMonNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     register: deploy
     shell: ceph-deploy --cluster {{ cluster_name }} new --public-network {{ public_network }} {% for host in CephMonNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: not dev


   - name: Cambia OSD poll size a 2
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     lineinfile:
      dest: /home/{{cephuser}}/{{ cluster_name }}/ceph.conf
      state: present
      regexp: '^osd pool default size'
      line: 'osd pool default size = 2'
     when: not dev

    
    # Add this poiny whe have:
    #
    # [global]
    # fsid = 9ea45fc6-4b06-424a-a261-f29faaf5a3c8
    # mon_initial_members = ceph1, ceph2
    # mon_host = 192.168.100.101,192.168.100.102
    # auth_cluster_required = cephx
    # auth_service_required = cephx
    # auth_client_required = cephx
    # osd pool default size = 2
    # public network = 192.168.100.0/255.255.255.0


    # Install Ceph packages on remote hosts. As a first step it installs yum-plugin-priorities in admin and other nodes using passwordless ssh and sudo
    # so that Ceph packages from upstream repository get more priority. It then detects the platform and distribution for the hosts and installs Ceph 
    # normally by downloading distro compatible packages if adequate repo for Ceph is already added. --release flag 
    # is used to get the latest release for installation. During detection of platform and distribution before installation, if it finds the 
    # distro.init to be sysvinit (Fedora, CentOS/RHEL etc), it doesn’t allow installation with custom cluster name and uses the default name 
    # ceph for the cluster.
    # If the user explicitly specifies a custom repo url with --repo-url for installation, anything detected from the configuration will be overridden
    # and the custom repository location will be used for installation of Ceph packages. 
    # If required, valid custom repositories are also detected and installed. 
    # In case of installation from a custom repo a boolean is used to determine the logic needed to proceed with a custom repo installation. 
    # A custom repo install helper is used that goes through config checks to retrieve repos (and any extra repos defined) and installs them. 
    # cd_conf is the object built from argparse that holds the flags and information needed to determine what metadata from the configuration is to 
    # be used. A user can also opt to install only the repository without installing Ceph and its dependencies by using --repo option.   
   
   - debug: msg="ceph deploy install --release {{ ceph_stable_release }} {% for host in CephAllNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}"  
                                                    
          
   - name: ceph-deploy install --release {{ ceph_stable_release }} {% for host in CephAllNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     register: deploy
     shell: ceph-deploy install --release {{ ceph_stable_release }} {% for host in CephAllNode %}{{ host }}{% if not loop.last %} {% endif %}{% endfor %}
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: not dev
    
    # Subcommand create-initial deploys for monitors defined in mon initial members under [global] section in Ceph configuration file, 
    # wait until they form quorum and then gatherkeys, reporting the monitor status along the process. If monitors don’t form quorum the 
    # command will eventually time out. 
    
   - name: Ejecuta ceph mon create-initial
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     shell: ceph-deploy mon create-initial
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: not dev

   - name: ceph-deploy osd prepare {% for c in cluster %} {% if c.osdnode is defined %}{% for osd in c.osdnode %}{% if osd.dev is defined %}{{ c.node }}:{{osd.dev}}{% if not loop.last %} {% endif %}{% endif %}{% endfor %}{% endif %}{% endfor %}
     become: yes
     become_method: "sudo"
     become_user: "{{ cephuser }}"
     shell: ceph-deploy osd prepare {% for c in cluster %} {% if c.osdnode is defined %}{% for osd in c.osdnode %}{% if osd.dev is defined %}{{ c.node }}:{{osd.dev}}{% if not loop.last %} {% endif %}{% endif %}{% endfor %}{% endif %}{% endfor %}
     args:
      chdir: /home/{{cephuser}}/{{ cluster_name }}
     when: not dev   
